{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import sklearn as sk\n",
    "from sklearn import metrics\n",
    "# http://stackoverflow.com/questions/35365007/tensorflow-precision-recall-f1-score-and-confusion-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.6 s, sys: 2.95 s, total: 44.6 s\n",
      "Wall time: 53.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all = pd.read_csv('./casted_data_norm.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293941, 604)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM</th>\n",
       "      <th>201508_AUM</th>\n",
       "      <th>201508_CARD_DELIN_AMT</th>\n",
       "      <th>201508_JCIC_CA_AMT</th>\n",
       "      <th>201508_JCIC_CASH_LIMIT</th>\n",
       "      <th>201508_JCIC_LAST_PAY</th>\n",
       "      <th>201508_JCIC_PAYABLE</th>\n",
       "      <th>201508_JCIC_PERM_LIMIT</th>\n",
       "      <th>201508_JCIC_PRE_BILLED</th>\n",
       "      <th>201508_JCIC_PRE_CASHED</th>\n",
       "      <th>...</th>\n",
       "      <th>201607_TSB_INSTL_PRD1_AMT</th>\n",
       "      <th>201607_TSB_INSTL_PRD2_AMT</th>\n",
       "      <th>201607_TSB_INSTL_PRD3_AMT</th>\n",
       "      <th>201607_TSB_LOL_AMT</th>\n",
       "      <th>201607_TSB_LOLFEE_AMT</th>\n",
       "      <th>201607_TSB_ONCECA_AMT</th>\n",
       "      <th>201607_TSB_SPD_AMT</th>\n",
       "      <th>RSP_AMT</th>\n",
       "      <th>RSP_FLG</th>\n",
       "      <th>RSP_FLG_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>5.505332</td>\n",
       "      <td>9.579003</td>\n",
       "      <td>9.289152</td>\n",
       "      <td>5.991465</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>9.047468</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>6.152733</td>\n",
       "      <td>11.506092</td>\n",
       "      <td>11.018859</td>\n",
       "      <td>6.393591</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>10.611129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>5.442418</td>\n",
       "      <td>4.663439</td>\n",
       "      <td>10.079288</td>\n",
       "      <td>6.142037</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>12.816523</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>6.327937</td>\n",
       "      <td>8.672144</td>\n",
       "      <td>8.816112</td>\n",
       "      <td>6.775366</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>8.513185</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>5.996452</td>\n",
       "      <td>8.668712</td>\n",
       "      <td>8.719481</td>\n",
       "      <td>6.532334</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>6.907755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 604 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NUM  201508_AUM  201508_CARD_DELIN_AMT  201508_JCIC_CA_AMT  \\\n",
       "0      1  -11.512925             -11.512925          -11.512925   \n",
       "1     10    9.047468             -11.512925          -11.512925   \n",
       "2    100  -11.512925             -11.512925          -11.512925   \n",
       "3   1000   12.816523             -11.512925          -11.512925   \n",
       "4  10000  -11.512925             -11.512925          -11.512925   \n",
       "\n",
       "   201508_JCIC_CASH_LIMIT  201508_JCIC_LAST_PAY  201508_JCIC_PAYABLE  \\\n",
       "0                5.505332              9.579003             9.289152   \n",
       "1                6.152733             11.506092            11.018859   \n",
       "2                5.442418              4.663439            10.079288   \n",
       "3                6.327937              8.672144             8.816112   \n",
       "4                5.996452              8.668712             8.719481   \n",
       "\n",
       "   201508_JCIC_PERM_LIMIT  201508_JCIC_PRE_BILLED  201508_JCIC_PRE_CASHED  \\\n",
       "0                5.991465              -11.512925              -11.512925   \n",
       "1                6.393591              -11.512925              -11.512925   \n",
       "2                6.142037              -11.512925              -11.512925   \n",
       "3                6.775366              -11.512925              -11.512925   \n",
       "4                6.532334              -11.512925              -11.512925   \n",
       "\n",
       "     ...      201607_TSB_INSTL_PRD1_AMT  201607_TSB_INSTL_PRD2_AMT  \\\n",
       "0    ...                     -11.512925                 -11.512925   \n",
       "1    ...                     -11.512925                 -11.512925   \n",
       "2    ...                     -11.512925                 -11.512925   \n",
       "3    ...                     -11.512925                 -11.512925   \n",
       "4    ...                     -11.512925                 -11.512925   \n",
       "\n",
       "   201607_TSB_INSTL_PRD3_AMT  201607_TSB_LOL_AMT  201607_TSB_LOLFEE_AMT  \\\n",
       "0                 -11.512925          -11.512925             -11.512925   \n",
       "1                 -11.512925          -11.512925             -11.512925   \n",
       "2                 -11.512925          -11.512925             -11.512925   \n",
       "3                 -11.512925          -11.512925             -11.512925   \n",
       "4                 -11.512925          -11.512925             -11.512925   \n",
       "\n",
       "   201607_TSB_ONCECA_AMT  201607_TSB_SPD_AMT  RSP_AMT  RSP_FLG  RSP_FLG_N  \n",
       "0             -11.512925          -11.512925        0        0          1  \n",
       "1             -11.512925           10.611129        0        0          1  \n",
       "2             -11.512925          -11.512925        0        0          1  \n",
       "3             -11.512925            8.513185        0        0          1  \n",
       "4             -11.512925            6.907755        0        0          1  \n",
       "\n",
       "[5 rows x 604 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = df_all.assign(RSP_FLG_N = 1-df_all.RSP_FLG)\n",
    "print(df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(df_all.columns.values) - 4)\n",
    "# 600 = 12 * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train(80%): 235152 20%: 58788\n",
      "(235152, 604)\n",
      "(58789, 604)\n"
     ]
    }
   ],
   "source": [
    "print('train(80%):', int(df_all.shape[0]*0.8), '20%:', int(df_all.shape[0]*0.2))\n",
    "train = df_all.iloc[:235152]; print(train.shape)\n",
    "test = df_all.iloc[235152:]; print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235152, 600)\n",
      "(58789, 600)\n"
     ]
    }
   ],
   "source": [
    "train_x = train.iloc[:, 1:-3].as_matrix(); print(train_x.shape)\n",
    "test_x = test.iloc[:, 1:-3].as_matrix(); print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (235152, 2)\n",
      "<class 'numpy.ndarray'> (58789, 2)\n"
     ]
    }
   ],
   "source": [
    "train_y = train[['RSP_FLG_N', 'RSP_FLG']].as_matrix()\n",
    "test_y = test[['RSP_FLG_N', 'RSP_FLG']].as_matrix()\n",
    "\n",
    "print(type(train_y), train_y.shape)\n",
    "print(type(train_y), test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert to tensor\n",
    "# tf_train_x = tf.convert_to_tensor(np.array(train_x), dtype=tf.float32)\n",
    "# tf_train_y = tf.convert_to_tensor(np.array(train_y), dtype=tf.float32)\n",
    "# tf_test_x = tf.convert_to_tensor(np.array(test_x), dtype=tf.float32)\n",
    "# tf_test_y = tf.convert_to_tensor(np.array(test_y), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try: \n",
    "#     del tf_train_x, tf_train_y, tf_test_x, tf_test_y\n",
    "# except:\n",
    "#     pass\n",
    "# gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape) + 1e-6\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "def model_text():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "## 建立 Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 3, 13, 128]\n"
     ]
    }
   ],
   "source": [
    "my_nn = tf.Graph()\n",
    "with my_nn.as_default():\n",
    "    with tf.name_scope('Data_Input'):\n",
    "        x_ = tf.placeholder(tf.float32, [None, 600], name=\"x_\")\n",
    "        y_ = tf.placeholder(tf.float32, [None, 2], name=\"y_\")\n",
    "        x_input = tf.reshape(x_, [-1, 12, 50, 1])\n",
    "\n",
    "    with tf.variable_scope(\"Wb_1\"):\n",
    "        W_conv1 = weight_variable([3, 3, 1, 256])\n",
    "        b_conv1 = bias_variable([256])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_input, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    with tf.variable_scope(\"Wb_2\"):\n",
    "        W_conv2 = weight_variable([3, 3, 256, 128])\n",
    "        b_conv2 = bias_variable([128])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = h_pool2.get_shape().as_list()\n",
    "    #[print(type(x)) for x in pool_shape]\n",
    "    pool_shape = [-1 if x is None else int(x) for x in pool_shape]\n",
    "    print(pool_shape)\n",
    "    \n",
    "    with tf.variable_scope(\"fc_1\"):\n",
    "        #pool_shape = h_pool2.get_shape().as_list()\n",
    "        W_fc1 = weight_variable([pool_shape[1]*pool_shape[2]*pool_shape[3], 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, pool_shape[1]*pool_shape[2]*pool_shape[3]])\n",
    "                                #[-1, 3 * 13 * 64])\n",
    "\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    ## Dropout\n",
    "    with tf.variable_scope(\"Dropout\"):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    ## Readout Layer\n",
    "    with tf.variable_scope(\"Output\"):\n",
    "        W_fc2 = weight_variable([1024, 2])\n",
    "        b_fc2 = bias_variable([2])\n",
    "        y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        y_conv = tf.nn.softmax(y)\n",
    "\n",
    "    with tf.variable_scope(\"Loss\"):\n",
    "        ratio = 5644 / (288297 + 5644)\n",
    "        class_weight = tf.constant([ratio, 1.0 - ratio])\n",
    "        weighted_logits = tf.mul(y_conv, class_weight)\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "\n",
    "    with tf.variable_scope(\"Train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(1e-2)\n",
    "        trainer = optimizer.minimize(cross_entropy)\n",
    "\n",
    "    ### 建立成效評估用的 Computational Graph\n",
    "    with tf.variable_scope(\"Accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    ### Init\n",
    "    with tf.name_scope(\"Initializer\"):\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.name_scope('Global_Step'):\n",
    "        global_step = tf.Variable(0, name=\"step\", trainable=False)\n",
    "    \n",
    "    ## Tensorboard Summary\n",
    "    summ_ce = tf.scalar_summary(\"Loss\", cross_entropy)\n",
    "    summ_accr = tf.scalar_summary(\"Accuracy\", accuracy)\n",
    "    summ_scalar = tf.merge_summary([summ_ce, summ_accr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 600)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0:3].shape)  # top 3 sample\n",
    "print(train_y[0:3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12, 50, 128)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=my_nn) as sess:\n",
    "    sess.run(init)\n",
    "    result_h_conv1 = sess.run(h_conv1, feed_dict={x_: train_x[0:3]})\n",
    "print(result_h_conv1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 25, 128)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=my_nn) as sess:\n",
    "    sess.run(init)\n",
    "    result_h_pool1 = sess.run(h_pool1, feed_dict={x_: train_x[0:3]})\n",
    "print(result_h_pool1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 25, 64)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=my_nn) as sess:\n",
    "    sess.run(init)\n",
    "    result_h_conv2 = sess.run(h_conv2, feed_dict={x_: train_x[0:3]})\n",
    "print(result_h_conv2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 13, 64)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=my_nn) as sess:\n",
    "    sess.run(init)\n",
    "    result_h_pool2 = sess.run(h_pool2, feed_dict={x_: train_x[0:3]})\n",
    "print(result_h_pool2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_fc1: (3, 1024) \n",
      " [[  55.61452866  136.28445435   34.31803894 ...,    0.            0.\n",
      "    54.7549324 ]\n",
      " [  40.84869003  111.62478638    0.         ...,    0.            0.            0.        ]\n",
      " [  58.3553009   128.57019043   27.76678276 ...,    0.            0.\n",
      "    51.42941666]]\n",
      "h_fc1: \n",
      " [[  24.22477722  146.90985107]\n",
      " [  38.75052643  139.56370544]\n",
      " [  37.02106857  131.60922241]]\n",
      "y_conv: (3, 2) \n",
      " [[  0.00000000e+00   1.00000000e+00]\n",
      " [  0.00000000e+00   1.00000000e+00]\n",
      " [  8.33492327e-42   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=my_nn) as sess:\n",
    "    sess.run(init)\n",
    "    result_h_fc1 = sess.run(h_fc1, feed_dict={x_: train_x[0:3]})\n",
    "    res_y = sess.run(y, feed_dict={x_: train_x[0:3], keep_prob: 1.})\n",
    "    result_y_conv = sess.run(y_conv, feed_dict={x_: train_x[0:3], keep_prob: 1.})\n",
    "print(\"h_fc1:\", result_h_fc1.shape, '\\n', result_h_fc1)\n",
    "print(\"h_fc1:\", '\\n', res_y)\n",
    "print(\"y_conv:\", result_y_conv.shape, \"\\n\", result_y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=my_nn) as sess:\n",
    "    sess.run(init)\n",
    "    batch_xs = train_x[0:5, ...]\n",
    "    batch_ys = train_y[0:5, ...]\n",
    "#     sess.run(trainer, \n",
    "#              feed_dict={x_: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "    _ = sess.run(tf.argmax(y_, 1), \n",
    "                               feed_dict={x_: test_x, y_: test_y, keep_prob: 0.5})\n",
    "#     valid_accurarcy = sess.run(accuracy, \n",
    "#                         feed_dict={x_: test_x, y_: test_y, keep_prob: 1})\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 235152\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "EVAL_FREQUENCY = 100\n",
    "PATIENCE = 50\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "#n_features = 600\n",
    "#n_classes = 2\n",
    "train_size = train_y.shape[0]; print(\"train size:\", train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/1050391/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/1050391/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (batch size 200, epoch 0.000), ce:0.328262, train-accu:0.985, valid-f1:0.000, valid-precision:0.000, valid-recall:0.000, 15.649 ms\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "Step 100 (batch size 200, epoch 0.085), ce:0.333262, train-accu:0.980, valid-f1:0.000, valid-precision:0.000, valid-recall:0.000, 1379.892 ms\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "Step 200 (batch size 200, epoch 0.170), ce:0.338262, train-accu:0.975, valid-f1:0.000, valid-precision:0.000, valid-recall:0.000, 1383.770 ms\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "step = 0\n",
    "best_accurarcy = 0\n",
    "tic = time.time()\n",
    "\n",
    "with tf.Session(graph=my_nn) as sess:\n",
    "    ### Build TensorBoard Writer\n",
    "    writer = tf.train.SummaryWriter(\"./logs\", my_nn)\n",
    "    merged = tf.merge_all_summaries()\n",
    "    saver = tf.train.Saver(max_to_keep=10)\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    sess.run(init); print('Initialized!')\n",
    "    \n",
    "    patience = PATIENCE\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tf.add_to_collection('Global_Step', global_step)\n",
    "    tf.add_to_collection('Train', trainer)\n",
    "    tf.add_to_collection('Accuracy', accuracy)\n",
    "    tf.add_to_collection('x_', x_)\n",
    "    tf.add_to_collection('y_', y_)\n",
    "\n",
    "    for step in range(int(NUM_EPOCHS * train_size) // BATCH_SIZE):\n",
    "        #batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        # Generate batches\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_xs = train_x[offset:(offset + BATCH_SIZE), ...]\n",
    "        batch_ys = train_y[offset:(offset + BATCH_SIZE), ...]\n",
    "        \n",
    "        sess.run(trainer, \n",
    "                 feed_dict={x_: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "        sess.run(tf.assign(global_step, step))\n",
    "\n",
    "        ## Print every EVAL_FREQUENCY\n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            ce = sess.run(cross_entropy,\n",
    "                          feed_dict={x_: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "            train_accurarcy = sess.run(accuracy, \n",
    "                                       feed_dict={x_: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "            # Write scalar summary to session graph\n",
    "#             result_scalar = sess.run(summ_scalar)\n",
    "            \n",
    "            pred = sess.run(y_conv, feed_dict={x_: test_x[:5000, ], y_: test_y[:5000,], keep_prob: 1})\n",
    "            y_pred = np.argmax(pred, 1)\n",
    "            y_true = np.argmax(test_y[:5000,],1)\n",
    "            precision = metrics.precision_score(y_true, y_pred, pos_label=1)\n",
    "            recall = metrics.recall_score(y_true, y_pred, pos_label=1)\n",
    "            f1 = metrics.f1_score(y_true, y_pred, pos_label=1)\n",
    "            \n",
    "#             print(\"Precision\", precision)\n",
    "#             print(\"Recall\", recall)\n",
    "#             print(\"f1_score\", f1)\n",
    "#             print(\"confusion_matrix\")\n",
    "#             print(metrics.confusion_matrix(y_true, y_pred))\n",
    "            \n",
    "            #writer.add_summary(result_scalar, step)\n",
    "            \n",
    "            print(\"Step %d (batch size %d, epoch %.3f), ce:%f, train-accu:%.3f, valid-f1:%.3f, valid-precision:%.3f, valid-recall:%.3f, %.3f ms\" % \\\n",
    "                  (step, \n",
    "                   BATCH_SIZE,\n",
    "                   float(step) * BATCH_SIZE / train_size,\n",
    "                   ce,\n",
    "                   train_accurarcy, \n",
    "                   f1,\n",
    "                   precision,\n",
    "                   recall,\n",
    "                   1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "            print(pred)\n",
    "            \n",
    "            if f1 > best_accurarcy:\n",
    "                patience = PATIENCE\n",
    "                best_accurarcy = f1\n",
    "                print(\"save\", end=\" \")\n",
    "                saver.save(sess, \"./model_1/model_conv.ckpt\", global_step=global_step)\n",
    "                print(\">>> model saved!\")\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print(\"early stop\")\n",
    "                    break\n",
    "\n",
    "print(\"---------------------\")\n",
    "print(\"time elapse: %s\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore model\n",
    "\n",
    "+ http://stackoverflow.com/a/40765759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=my_nn) as sess:\n",
    "    new_saver = tf.train.import_meta_graph('./model_1/model_conv.ckpt-1700.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./model_1/'))\n",
    "#     saver.restore(sess, \"./model_1/model_conv.ckpt-1700\")\n",
    "    ce = cross_entropy.eval(feed_dict={x_: batch_xs, y_: batch_ys, keep_prob: 1}, session=sess)\n",
    "    y_pred = y.eval(feed_dict={x_: test_x, y_: batch_ys, keep_prob: 1}, session=sess)\n",
    "    print(\"cross entropy:\", ce)\n",
    "    print(\"y_softmax:\\n\", y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Softmax Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 600])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "W = tf.Variable(tf.zeros([600, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "y = tf.matmul(x,W) + b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 176364\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "train_size = train_y.shape[0]; print(\"train size:\", train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977972\n",
      "Softmax: [[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "#     for step in range(int(2 * train_size) // BATCH_SIZE):\n",
    "#         offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "#         batch_xs = train_x[offset:(offset + BATCH_SIZE), ...]\n",
    "#         batch_ys = train_y[offset:(offset + BATCH_SIZE), ...]\n",
    "    for step in range(int(NUM_EPOCHS * train_size) // BATCH_SIZE):\n",
    "        # Generate batches\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_xs = train_x[offset:(offset + BATCH_SIZE), ...]\n",
    "        batch_ys = train_y[offset:(offset + BATCH_SIZE), ...]\n",
    "\n",
    "        train_step.run(feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    softmax = tf.nn.softmax(y)\n",
    "    print('Accuracy:', accuracy.eval(feed_dict={x: test_x, y_: test_y}))\n",
    "    print('Softmax:', softmax.eval(feed_dict={x: test_x, y_: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
